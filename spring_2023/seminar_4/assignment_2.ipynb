{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2  \n",
    "Implement Hidden Markov Model with supervised training and Viterbi algorithm for finding the most probable sequence of hidden states.  \n",
    "Use [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) for estimation of probabilities.  \n",
    "Apply the developed model to the problems:\n",
    "* part of speech tagging\n",
    "* spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM model for 3-grams:\n",
    "\n",
    "$P(x_1, .., x_T, y_1, .., y_T, y_{T+1}) = \\prod_{t=1}^{T+1} q(y_t | y_{t-2}, y_{t-1}) \\prod_{t=1}^T e(x_t | y_t)$\n",
    "\n",
    "$x_1, .., x_T$ - sequence of observed states of length T  \n",
    "$y_1, .., y_T$ - sequence of hidden states of length T  \n",
    "$q(i | u, v) = \\frac {count(u, v, i)} {count(u, v)} $ - transition probability   \n",
    "$e(x_k | i) = \\frac {count(i, x_k)} {count(i)}$ - emission probability  \n",
    "$A_{i,j} = A_{(u,v), j} = q(i | u, v)$ - transition matrix  \n",
    "$B_{j,k} = e(x_k | j)$ - emission matrix  \n",
    "\n",
    "We assume, that $y_{T+1} = TERM$, and $y_0 = y_{-1} = START$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Implement HMM (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    START = '*'\n",
    "    TERM = '$'\n",
    "    REST = '$REST$' # to deal with observed states who have never appeared in train dataset.\n",
    "        \n",
    "    def cond_idx(self, u, v):\n",
    "        return u + v*self.h_dim\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X - list of lists, observed states\n",
    "        y - list of lists, hidden states\n",
    "        estimate elements of A, B matrices from train sequence. \n",
    "        \"\"\"\n",
    "        \n",
    "        #######################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # self.hidden_idx2state = # lisf of unique hidden states in train sequence + [START, TERM]\n",
    "        # self.hidden_states = # from state name to state index in hidden_idx2state\n",
    "        # self.h_dim = number of hidden states\n",
    "        \n",
    "        # self.observed_idx2state = # lisf of unique observed states in train sequence + [REST]\n",
    "        # self.observed_states = # from state name to state index in hidden_idx2state\n",
    "        # self.o_dim = number of observed states\n",
    "        \n",
    "        #######################       \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        # estimate emission matrix\n",
    "        # YOUR CODE HERE\n",
    "        # self.B = sparse csr matrix of shape (h_dim, o_dim)\n",
    "        #######################\n",
    "        \n",
    "        self.B_rowsum = np.ravel(self.B.sum(axis=1))\n",
    "        \n",
    "        \n",
    "        ########################\n",
    "        # transition matrix\n",
    "        # YOUR CODE HERE\n",
    "        # self.A = dense matrix of shape (h_dim **2, h_dim)\n",
    "        # remember about padding for sequence of hidden states, eg {a, b} -> {START, START, a, b, TERM}\n",
    "        ########################\n",
    "        \n",
    "        self.A_rowsum = np.ravel(self.A.sum(axis=1))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def tr_prob(self, i, j):\n",
    "        \"\"\"\n",
    "        A_ij = q(j | i) = q(j| u, v) with Laplace smoothing\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        # result = smoothed probability\n",
    "        ########################\n",
    "        return result\n",
    "    \n",
    "    def em_prob(self, i, j):\n",
    "        \"\"\"\n",
    "        B_jk = e(x_k| j) with Laplace smoothing\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # YOUR CODE HERE\n",
    "        # result = smoothed probability\n",
    "        ########################\n",
    "        return result\n",
    "        \n",
    "    def o_state(self, x):\n",
    "        \"\"\"\n",
    "        return index of obseved state\n",
    "        \"\"\"\n",
    "        return self.observed_states.get(x, self.observed_states[self.REST])\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most probable sequence of hidden states for every sequence of observed states\n",
    "        X - list of lists\n",
    "        \"\"\"\n",
    "        y_pred = [self._viterbi(seq) for seq in tqdm(X)]\n",
    "        return y_pred \n",
    "            \n",
    "    def _viterbi(self, X):\n",
    "        \"\"\"\n",
    "        X - list of observables\n",
    "        product of probabilities usually is not numerically stable\n",
    "        remember, that log(ab) = log(a) + log(b) and argmax[log(f(x))] = argmax[f(x)]\n",
    "        \n",
    "        \"\"\"   \n",
    "        T = len(X)\n",
    "        \n",
    "        # pi[t, u, v] - max probability for any state sequence ending with x_t = v and x_{t-1} = u.\n",
    "        pi = np.zeros((T + 1, self.h_dim, self.h_dim))\n",
    "        # backpointers, bp[t, u, v] = argmax probability for any state sequence ending with x_t = v and x_{t-1} = u.\n",
    "        bp = np.zeros((T + 1, self.h_dim, self.h_dim), dtype=np.int)\n",
    "        \n",
    "        ###################\n",
    "        # fill tables pi and bp\n",
    "        # pi[t, u, v] = max_{w} [ pi[t-1, w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # bp[t, u, v] = argmax_{w} [ pi[t-1, w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # YOUR CODE HERE \n",
    "        ###################\n",
    "                    \n",
    "        term_idx = self.hidden_states[self.TERM]\n",
    "        \n",
    "        ###################\n",
    "        # r(u,v) = pi[T, u, v] * q(TERM | u, v)\n",
    "        # find argmax_{u, v} r(u, v)\n",
    "        # YOUR CODE HERE\n",
    "        # u, v = \n",
    "        ###################\n",
    "\n",
    "        h_states = [v, u]\n",
    "        ###################\n",
    "        # rollback backpointers\n",
    "        # y_{t-2} = bp[t, y_{t-1}, y_t]\n",
    "        # h_states is a reversed sequence of hidden states\n",
    "        # YOUR CODE HERE\n",
    "        # h_states = \n",
    "            \n",
    "        ###################        \n",
    "            \n",
    "        return [self.hidden_idx2state[i] for i in reversed(h_states[:T])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Part of speech tagging (20%)\n",
    "\n",
    "Build Part-of-Speech tagging model using HMM. Estimate accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/denaas/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "tags:  NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n"
     ]
    }
   ],
   "source": [
    "data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:3010]\n",
    "\n",
    "X_train = [[x[0] for x in y] for y in data]\n",
    "y_train = [[x[1] for x in y] for y in data]\n",
    "\n",
    "X_test = [[x[0] for x in y] for y in test_data]\n",
    "y_test = [[x[1] for x in y] for y in test_data]\n",
    "\n",
    "print('sentence: ', \" \".join(X_train[0]))\n",
    "print('tags: ', \" \".join(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):       \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7488789237668162\n",
      "CPU times: user 1min 7s, sys: 128 ms, total: 1min 7s\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hh = HMM().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy must be > 0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Vectorized viterbi (20%)\n",
    "Our currrent implementation of Viterbi is too slow. Let's make it vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmmVectorized(HMM):\n",
    "    \n",
    "    def _viterbi(self, X):\n",
    "        \"\"\"\n",
    "        Vectorized version of Viterbi. Let's speed up!\n",
    "        X - list of observables\n",
    "        \"\"\"   \n",
    "        T = len(X)\n",
    "        \n",
    "        # One may notice, at every step t we only need pi[t-1, u, v] = pi_prev[u,v] to compute pi[t, u, v] = pi_curr[u,v]\n",
    "        pi_prev = np.zeros((self.h_dim, self.h_dim))\n",
    "        \n",
    "        # backpointers\n",
    "        bp = np.zeros((T + 1, self.h_dim, self.h_dim), dtype=np.int)\n",
    "        \n",
    "        a_rowsum = self.A_rowsum.reshape(self.h_dim, self.h_dim)\n",
    "        \n",
    "        ###################\n",
    "        # fill pi and bp\n",
    "        # pi_curr[u, v] = max_{w} [ pi_prev[w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # bp[t, u, v] = argmax_{w} [ pi_prev[w, u] * q(v| w, u) * e(x_k| v) ]\n",
    "        # don't use tr_prob() and em_prob(), apply laplace smoothing directly here\n",
    "        # YOUR CODE HERE\n",
    "        ###################\n",
    "        \n",
    "        term_idx = self.hidden_states[self.TERM]\n",
    "        \n",
    "        ###################\n",
    "        # r(u,v) = pi[T, u, v] * q(TERM | u, v)\n",
    "        # find argmax_{u, v} r(u, v)\n",
    "        # express r(u,v) as matrix additions\n",
    "        # YOUR CODE HERE\n",
    "        # u, v = \n",
    "        ###################\n",
    "        \n",
    "        h_states = [v, u]\n",
    "        ###################\n",
    "        # rollback backpointers\n",
    "        # y_{t-2} = bp[t, y_{t-1}, y_t]\n",
    "        # h_states is a reversed sequence of hidden states\n",
    "        # YOUR CODE HERE\n",
    "        # h_states = \n",
    "            \n",
    "        ###################\n",
    "        \n",
    "        return [self.hidden_idx2state[i] for i in reversed(h_states[:T])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a larger test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "tags:  NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n"
     ]
    }
   ],
   "source": [
    "data = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3000:3300]\n",
    "\n",
    "X_train = [[x[0] for x in y] for y in data]\n",
    "y_train = [[x[1] for x in y] for y in data]\n",
    "\n",
    "X_test = [[x[0] for x in y] for y in test_data]\n",
    "y_test = [[x[1] for x in y] for y in test_data]\n",
    "\n",
    "print('sentence: ', \" \".join(X_train[0]))\n",
    "print('tags: ', \" \".join(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:46<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8424766977363515\n",
      "CPU times: user 46.7 s, sys: 104 ms, total: 46.8 s\n",
      "Wall time: 46.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hh = HmmVectorized().fit(X_train, y_train)\n",
    "y_pred = hh.predict(X_test)\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your accuracy must be > 0.83 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data of true_char corrupted\\_char build spelling correction model using HMM.    \n",
    "There are 2 datatsets (spelling10.txt, spelling20.txt) with 10% and 20% corruption probability respectively.  \n",
    "Each dataset contains 30556 words. Use first 28000 for training and the rest for testing. Estimate accuracy on the test subset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4. Spelling correction (20%)\n",
    "You should get > 92% accuracy on spelling10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task5. Spelling correction (20%)\n",
    "You should get > 88% accuracy on spelling20 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
