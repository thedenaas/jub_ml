{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b226600",
   "metadata": {},
   "source": [
    "# Learning to Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f34eb0",
   "metadata": {},
   "source": [
    "## 1 Problem Statement\n",
    "\n",
    "Let $Q = \\{q_1 ... q_m\\}$ - set of queries,  \n",
    "$D_i = \\{d_{i,1} ... d_{i,n_q} \\}$ - set of relevant documents for $q_i$,  \n",
    "$y_i = \\{y_{i,1} ... y_{i,n_q} \\}$ - set of relevance scores for $D_i$,  \n",
    "$x_{i,j} = \\phi(q_i, d_{i,j})$ - features for query $q_i$ and document $d_{i,j}$,  \n",
    "$x_i = \\{x_{i,j}\\}$ - features for query $q_i$ and documents $D_i$,  \n",
    "$f: R^d \\rightarrow R$ - ranking model,  \n",
    "$s_{i,j} = f(x_{i,j})$ - predicted ranking scores.  \n",
    "\n",
    "\n",
    "We need to train a model, which predicts a correct order of documents, sorted from high relevant to low relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafe6c6",
   "metadata": {},
   "source": [
    "## 2 Evaluation Metrics\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "$$Pr@k = \\frac {|relevant| \\cap |retrieved|} {|retrieved|}$$\n",
    "\n",
    "### Recall@k\n",
    "\n",
    "$$Re@k = \\frac {|relevant| \\cap |retrieved|} {|relevant|}$$\n",
    "\n",
    "\n",
    "### F1@k\n",
    "\n",
    "$$F1@k = \\frac {Pr@k * Re@k} {Pr@k + Re@k}$$\n",
    "\n",
    "\n",
    "### MAP@k\n",
    "\n",
    "$$AP@k = \\sum_K (Re@k - Re@[k-1]) \\cdot Pr@k $$\n",
    "$$MAP@k =  \\frac {1} {Q} \\sum_{q = 1}^Q AP(q)$$\n",
    "\n",
    "### MRR@k\n",
    "\n",
    "$$MRR = \\frac 1 {Q} \\sum^{Q}_{i = 1} \\frac 1 {rank_i}$$\n",
    "\n",
    "### NDCG@k\n",
    "\n",
    "$$ DCG@k = \\sum_{i=1}^n \\frac {2^{rel_i} - 1} {\\log_2 (i + 1)} $$\n",
    "\n",
    "$$ NDCG@k = \\frac {DCG@k} {IdealDCG@k} $$\n",
    "\n",
    "### Kendal rank correlation coefficient\n",
    "\n",
    "$$ \\tau = \\frac {(\\# concordant\\ pairs) - (\\# discordant\\ pairs)} {\\binom {n} {2}} $$\n",
    "$\\tau \\in [-1, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba4447",
   "metadata": {},
   "source": [
    "### Q\n",
    "\n",
    "* Why can't we optimize evaluation metrics directly?\n",
    "\n",
    "All evaluation metrics are either flat or discontinious.\n",
    "\n",
    "* Can we use classifier to solve ranking problem?\n",
    "\n",
    "We can predict probability of a document been top-1, but it is not very effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b734c9a",
   "metadata": {},
   "source": [
    "## 3 Pointwise Models\n",
    "\n",
    "We want to correctly predict ranking score for each (query, document) pair.\n",
    "\n",
    "$$  \\sum_{q=1}^Q \\sum_{j=1}^D L(f( {x}_{q,j}),\\ y_{q,j}) \\to \\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac25fb32",
   "metadata": {},
   "source": [
    "## 4 Pairwise Models\n",
    "\n",
    "$$ \\sum_{q=1}^Q \\sum_{i, j:\\ y_{q,i} \\gt y_{q,j}} L(f(x_{q,i}) - f(x_{q,j})) \\to \\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087b2ba",
   "metadata": {},
   "source": [
    "## RankNet\n",
    "\n",
    "Consider 2 documents for a single query:\n",
    "\n",
    "Let $P_{ij}$ - model prediction, that $f(x_i) > f(x_j)$,  \n",
    "$\\bar P_{ij}$ - ground truth,  \n",
    "$o_{ij} = f(x_i) - f(x_j)$, \n",
    "\n",
    "Loss between $i$-th and $j$-th document for query $q$:\n",
    "$$ L_{ij} = L(o_{ij}) = -\\bar P_{ij} \\log P_{ij} - (1 - \\bar P_{ij}) \\log(1 - P_{ij}) $$\n",
    "\n",
    "$$ P_{ij} = \\frac {e^{o_{ij}}} {1 + e^{o_{ij}}} $$\n",
    "\n",
    "$$ L_{ij} = -\\bar P_{ij} o_{ij} + \\log(1 + e^{o_{ij}}) $$\n",
    "\n",
    "\n",
    "Properties:\n",
    "* linear asymptotics - robust to noise\n",
    "* reduce of pairwise errors does not always imply increase in some ranking metric \n",
    "\n",
    "Q:\n",
    "* Monotonic behavior of $P_{ij}$  \n",
    "* What is the full expression for loss?\n",
    "* Asymptotic complexity $O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0d40c",
   "metadata": {},
   "source": [
    "## 5 Listwise Models\n",
    "\n",
    "$$ \\sum_{q=1}^Q L(\\{f(x_{q,j})\\}^{n_q}_{j=1},\\ \\{y_{q,j}\\}^{n_q}_{j=1}) \\to \\min $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b8bb5",
   "metadata": {},
   "source": [
    "## ListNet\n",
    "\n",
    "Consider permutation $\\pi = \\{ \\pi(1) ... \\pi(n)\\}$\n",
    "\n",
    "Let $\\phi$ be a strictly positive, monotonically increasing function.  \n",
    "$\\phi(s) = e^s$,  \n",
    "\n",
    "Probability of permutation \n",
    "$$ P_s (\\pi) = \\prod^n_{j = 1} \\frac {\\phi(s_{\\pi(j)})} {\\sum^n_{k = j} \\phi(s_{\\pi(k)})} $$\n",
    "\n",
    "\n",
    "For example, \n",
    "\n",
    "$$ P_s (\\pi) = \\prod^n_{j = 1} \\frac {\\phi(s_{\\pi(j)})} {\\sum^n_{k = j} \\phi(s_{\\pi(k)})} = \\frac {\\phi(s_1)} {\\phi(s_1) + \\phi(s_2) + \\phi(s_3)} \\cdot \\frac {\\phi(s_2)} {\\phi(s_2) + \\phi(s_3)} \\cdot \\frac {\\phi(s_3)} {\\phi(s_3)} $$\n",
    "\n",
    "! \"The permutations sorted by ranking function are most likely to occur\"\n",
    "\n",
    "This probability measure over permutations induce a probability distribution\n",
    "$$ \\sum_{\\pi \\in \\Omega_{\\pi}} P_s (\\pi) = 1 $$\n",
    "\n",
    "\n",
    "Top one probability\n",
    "\n",
    "$$ P_s(j) = \\sum_{\\pi(1) = j,\\ \\pi \\in \\Omega_n} P_s(\\pi)  =  \\frac {\\phi(s_j)} {\\sum^n_{k = 1} \\phi(s_k)} $$\n",
    "\n",
    "\n",
    "Loss function\n",
    "\n",
    "$$ L(y^{(i)}, z^{(i)}) = -\\sum^n_{j = 1} P_{y^{(i)}}(j) \\log(P_{z^{(i)}}(j)) $$\n",
    "\n",
    "### Q:\n",
    "\n",
    "* Properties of $P_s (\\pi)$\n",
    "* Properties of $P_s(j)$\n",
    "* Can we use another loss function in this model? \n",
    "Yes, for example KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a855a3ca",
   "metadata": {},
   "source": [
    "## 6 LambdaRank\n",
    "\n",
    "Original RankNet Loss is rewritten:\n",
    "\n",
    "$ \\bar P_{ij} = \\frac {1 + S_{ij}} 2 $,  \n",
    "$ S_{ij} \\in [-1, 1]$,  \n",
    "$ p(s_i > s_j) = \\sigma (\\alpha(s_i - s_j))$\n",
    "\n",
    "$$ L_{ij} = -\\bar P_{ij} \\log P_{ij} - (1 - \\bar P_{ij}) \\log(1 - P_{ij}) $$ \n",
    "\n",
    "$$ L_{ij} = -\\frac {1 + S_{ij}} 2 \\log (1 + e^{-\\alpha(s_i-s_j)}) - \\frac {1 - S_{ij}} 2 (-\\alpha(s_i-s_j) - \\log (1 + e^{-\\alpha(s_i-s_j)})) $$ \n",
    "\n",
    "$$ L_{ij} = \\frac {1 - S_{ij}} 2 \\alpha(s_i-s_j) + \\log (1 + e^{\\alpha(s_j-s_i))}) $$ \n",
    "\n",
    "Now loss function is symmetric\n",
    "\n",
    "\n",
    "$$ \\dfrac{\\partial L\\left(s_{i}-s_{j}\\right)}{\\partial s_{i}}=-\\dfrac{\\partial L\\left(s_{i}-s_{j}\\right)}{\\partial s_{j}}=\\alpha\\left(\\frac {1 - S_{ij}} 2-\\dfrac{1}{1+e^{\\alpha\\left(s_{i}-s_{j}\\right)}}\\right) = \\lambda_{ij}  $$\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial w}=\\sum_{(i,j) \\in D} \\dfrac{\\partial L}{\\partial s_{i}} \\dfrac{\\partial s_{i}}{\\partial w}+ \\dfrac{\\partial L}{\\partial s_{j}} \\dfrac{\\partial s_{j}}{\\partial w}\n",
    "= \\sum_{(i,j) \\in D} \\lambda_{ij} (\\dfrac{\\partial s_{i}}{\\partial w}-\\dfrac{\\partial s_{j}}{\\partial w}) $$\n",
    "\n",
    "Rewrite more efficient\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial w} = \\sum_{i} \\dfrac{\\partial s_{i}}{\\partial w} (\\sum_{(i,j) \\in D} \\lambda_{ij} - \\sum_{(j,i) \\in D} \\lambda_{ji}) $$\n",
    "\n",
    "Let $$\\lambda_{i}=\\sum_{j:\\{i, j\\} \\in D} \\lambda_{i j}-\\sum_{j:\\{j, i\\} \\in D} \\lambda_{i j} $$\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial w} = \\sum_{i} \\dfrac{\\partial s_{i}}{\\partial w} \\lambda_i $$\n",
    "\n",
    "$\\lambda_i$ can be loosely interpreted as a \"force\", which moves the document up or down in the list.\n",
    "\n",
    "Choice of $\\lambda$\n",
    "\n",
    "$$ DCG = \\sum_{i} \\dfrac{2^{\\mathrm{rel}_{i}}-1}{\\log _{2}(i+1)} $$\n",
    "\n",
    "$$ \\lambda'_{ij}=\\lambda_{ij}|\\Delta NDCG_{ij}|= \\left(\\dfrac{1}{1+e^{s_{i}-s_{j}}}\\right)\\left(2^{\\mathrm{rel}_{i}}-2^{\\mathrm{rel}_{j}}\\right)\\left(\\dfrac{1}{\\log_2 (i+1)}-\\dfrac{1}{\\log_2 (j+1)}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c95852",
   "metadata": {},
   "source": [
    "## LambdaMART\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "$$ F_{N}(x)=\\sum_{i=1}^{N} \\alpha_{i} f_{i}(x) $$\n",
    "\n",
    "We want to predict gradient\n",
    "\n",
    "$$ \\bar{y}_{i}=-\\left[\\dfrac{\\partial L\\left(y_{i}, F\\left(x_{i}\\right)\\right.}{\\partial F\\left(x_{i}\\right)}\\right]_{F(x)=F_{m-1}(x)} $$\n",
    "\n",
    "It is very similar to\n",
    "\n",
    "$$ \\lambda_{i} = \\sum_{j \\in P_{i}} \\dfrac{\\partial L\\left(s_{i}, s_{j}\\right)}{\\partial s_{i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7975e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c804b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
